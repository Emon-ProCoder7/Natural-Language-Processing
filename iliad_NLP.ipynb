{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering Insights from Homer's Iliad with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novels and text contain insights into ideologies and places that are often originally unknown to the reader. By reading a written piece, we uncover the opinions of the author on their chosen topic and come to understand both the topic and how the author thinks.\n",
    "\n",
    "In this project I've performed a natural language parsing analysis to gain deeper insight into [Homer’s The Iliad!](http://www.gutenberg.org/ebooks/6130) \n",
    "\n",
    "One of the beauties of natural language parsing with regular expressions is the ability to gain insight into lengthy pieces of text without a formal read!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>\n",
    "***Project Goal***\n",
    "- Finding out the main topics of discussion in the novel and can begin to discern some of the author’s thoughts and beliefs \n",
    ">\n",
    ">    \"Without Actually Reading The Novel\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, RegexpParser\n",
    "from tokenize_words import word_sentence_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom counter Function\n",
    "Two functions `np_chunk_counter()`, `vp_chunk_counter()` that returns the 30 most common *Noun Phrase-chunks* and *Verb Phrase-chunks* from a list of chunked sentences is defined here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def np_chunk_counter(chunked_sentences):\n",
    "    chunks = list()\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "    chunk_counter = Counter()\n",
    "    for chunk in chunks:\n",
    "        chunk_counter[chunk] += 1\n",
    "    return chunk_counter.most_common(30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vp_chunk_counter(chunked_sentences):\n",
    "    chunks = list()\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "    chunk_counter = Counter()\n",
    "    for chunk in chunks:\n",
    "        chunk_counter[chunk] += 1\n",
    "    return chunk_counter.most_common(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Preprocessing Text Data\n",
    "\n",
    "The Iliad, named `the_iliad.txt`, sourced from [Project Gutenberg](https://www.gutenberg.org/). Here is my [Text File](the_iliad.txt) of that novel. I've converted it to lowercase in my workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'appears', 'as', 'the', 'enunciator', 'of', 'opinions', 'as', 'different', 'in', 'their', 'tone', 'as', 'those', 'of', 'the', 'writers', 'who', 'have', 'handed', 'them', 'down', '.']\n"
     ]
    }
   ],
   "source": [
    "text = open('the_iliad.txt', encoding = 'utf-8').read().lower()\n",
    "\n",
    "\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n",
    "\n",
    "# Single tokenized sentence\n",
    "single_word_tokenized_sentence = word_tokenized_text[100]\n",
    "print(single_word_tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('but', 'CC'), ('this', 'DT'), ('when', 'WRB'), ('time', 'NN'), ('requires.', 'NN'), ('--', ':'), ('it', 'PRP'), ('now', 'RB'), ('remains', 'VBZ'), ('we', 'PRP'), ('launch', 'VBP'), ('a', 'DT'), ('bark', 'NN'), ('to', 'TO'), ('plough', 'VB'), ('the', 'DT'), ('watery', 'NN'), ('plains', 'VBZ'), (',', ','), ('and', 'CC'), ('waft', 'VBD'), ('the', 'DT'), ('sacrifice', 'NN'), ('to', 'TO'), ('chrysa', 'VB'), (\"'s\", 'POS'), ('shores', 'NNS'), (',', ','), ('with', 'IN'), ('chosen', 'JJ'), ('pilots', 'NNS'), (',', ','), ('and', 'CC'), ('with', 'IN'), ('labouring', 'JJ'), ('oars', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagged_text = []\n",
    "for sentence in word_tokenized_text:\n",
    "  pos_tagged_text.append(pos_tag(sentence))\n",
    "\n",
    "\n",
    "\n",
    "# Single sentence with Parts of Speecj tag\n",
    "single_pos_sentence = pos_tagged_text[707]\n",
    "\n",
    "print(single_pos_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk Grammar\n",
    "\n",
    "- I have defined **Noun-phrase** (Determiner + Adj. + Noun) and\n",
    "- **Verb Pharse** (Noun-phrase + Verb + Adv.) chunk grammer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk Sentence\n",
    "\n",
    "- I have imported the **Noun Phrases** of the novel in `np_chunked_text` and \n",
    "- The **Verb Phrases** of the novel in `vp_chunked_text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_chunked_text = []\n",
    "vp_chunked_text = []\n",
    "for sentence in pos_tagged_text:\n",
    "  np_chunked_text.append(np_chunk_parser.parse(sentence))\n",
    "  vp_chunked_text.append(vp_chunk_parser.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novel Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `np_chunk_counter()` and `vp_chunk_counter()` that I defined earlier, returns the 30 most common **NP-chunks** and **VP-chunks** from a list of chunked sentences. I've printed the `most_common_np_chunks` and `most_common_vp_chunks` in the editor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('hector', 'NN'),), 322), ((('i', 'NN'),), 277), ((('jove', 'NN'),), 257), ((('troy', 'NN'),), 208), ((('vain', 'NN'),), 195), ((('war', 'NN'),), 193), ((('son', 'NN'),), 170), ((('thou', 'NN'),), 158), ((('the', 'DT'), ('plain', 'NN')), 157), ((('the', 'DT'), ('field', 'NN')), 154), ((('the', 'DT'), ('ground', 'NN')), 138), ((('death', 'NN'),), 134), ((('hand', 'NN'),), 134), ((('greece', 'NN'),), 128), ((('heaven', 'NN'),), 127), ((('fate', 'NN'),), 127), ((('thee', 'NN'),), 122), ((('breast', 'NN'),), 121), ((('the', 'DT'), ('trojan', 'NN')), 120), ((('the', 'DT'), ('god', 'NN')), 119), ((('the', 'DT'), ('war', 'NN')), 117), ((('the', 'DT'), ('greeks', 'NN')), 116), ((('blood', 'NN'),), 115), ((('homer', 'NN'),), 112), ((('the', 'DT'), ('king', 'NN')), 105), ((('rage', 'NN'),), 103), ((('force', 'NN'),), 103), ((('care', 'NN'),), 99), ((('head', 'NN'),), 98), ((('man', 'NN'),), 97)]\n"
     ]
    }
   ],
   "source": [
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(most_common_np_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- Looking at `most_common_np_chunks`, it can be identified characters of importance in the text such as **hector** and **jove** based on their frequency. Additionally a location of importance, **troy**, is mentioned often. **A theme of war** can also implied by its high frequency count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(((\"'t\", 'NN'), ('is', 'VBZ')), 19), ((('i', 'NN'), ('am', 'VBP')), 11), (((\"'t\", 'NN'), ('was', 'VBD')), 11), ((('the', 'DT'), ('hero', 'NN'), ('said', 'VBD')), 9), ((('i', 'NN'), ('know', 'VBP')), 8), ((('i', 'NN'), ('saw', 'VBD')), 8), ((('the', 'DT'), ('scene', 'NN'), ('lies', 'VBZ')), 7), ((('i', 'NN'), ('was', 'VBD')), 6), ((('confess', 'NN'), (\"'d\", 'VBD')), 6), ((('the', 'DT'), ('scene', 'NN'), ('is', 'VBZ')), 6), ((('view', 'NN'), (\"'d\", 'VBD')), 5), ((('i', 'NN'), ('felt', 'VBD')), 5), ((('i', 'NN'), ('bear', 'VBP')), 5), ((('hector', 'NN'), ('is', 'VBZ')), 5), ((('vain', 'NN'), ('was', 'VBD')), 5), ((('homer', 'NN'), ('was', 'VBD')), 4), ((('i', 'NN'), ('have', 'VBP')), 4), ((('hunger', 'NN'), ('was', 'VBD')), 4), ((('glory', 'NN'), ('lost', 'VBN')), 4), ((('i', 'NN'), ('see', 'VBP')), 4), ((('war', 'NN'), ('be', 'VB')), 4), ((('the', 'DT'), ('weapon', 'NN'), ('stood', 'VBD')), 4), ((('i', 'NN'), ('go', 'VBP')), 4), ((('the', 'DT'), ('silence', 'NN'), ('broke', 'VBD')), 4), ((('the', 'DT'), ('trojan', 'NN'), ('bands', 'VBZ')), 4), ((('father', 'NN'), ('gave', 'VBD')), 4), ((('i', 'NN'), ('deem', 'VBP')), 4), ((('minerva', 'NN'), ('repressing', 'VBG')), 3), ((('thetis', 'NN'), ('calling', 'VBG')), 3), ((('thetis', 'NN'), ('entreating', 'VBG')), 3)]\n"
     ]
    }
   ],
   "source": [
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print(most_common_vp_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- Looking at `most_common_vp_chunks`, it appears that verb phrases that is defined in the chunk grammar do not appear as often in The Iliad as noun phrases. \n",
    "\n",
    "This can indicate a different style of writing taken by the author that does not follow traditional grammatical style (i.e. poetry). Even when chunks are not found, their absence can give insights!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
